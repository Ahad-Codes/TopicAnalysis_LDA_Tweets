{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_sentences = [\n",
    "    \"the COVID-19 vaccine rollout has been effective in reducing the number of cases.\",\n",
    "    \"New variants of the coronavirus are causing a lot of concern among health experts.\",\n",
    "    \"Many people are hesitant about the COVID vaccine due to misinformation.\",\n",
    "    \"Hospitals are overwhelmed due to the surge in COVID-19 cases.\",\n",
    "    \"The pandemic has led to a significant increase in mental health issues worldwide.\",\n",
    "    \"Researchers are studying the long-term effects of COVID-19 on the human body.\",\n",
    "    \"Remote work has become the new norm since the COVID pandemic started.\",\n",
    "    \"Many countries are implementing stricter lockdowns to control the spread of COVID-19.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "\n",
    "    def __init__(self, docs, k, iterations, alpha, beta):\n",
    "        self.k = k\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.docs = self.preprocess_tweets(docs)\n",
    "\n",
    "        self.num_docs = len(docs)\n",
    "\n",
    "        # Setting vocab and id mappings\n",
    "        vectorizer = CountVectorizer(max_features=50000)\n",
    "        print(self.docs)\n",
    "        vectorizer.fit(self.docs)\n",
    "        self.vocab = vectorizer.get_feature_names_out()\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # Essentially gives us a mapping from word to id and vice versa (used for updating counts)\n",
    "        self.word2id = vectorizer.vocabulary_\n",
    "        print(self.word2id)\n",
    "        self.id2word = {idx: word for word, idx in self.word2id.items()}\n",
    "        print(self.id2word)\n",
    "        \n",
    "        # Counts needed for gibbs sampling\n",
    "        self.doc_topic_count = np.zeros((self.num_docs, self.k))\n",
    "        self.topic_word_count = np.zeros((self.k, len(self.vocab)))\n",
    "        self.topic_total_counts = np.zeros(self.k)\n",
    "        self.doc_lengths = np.zeros(self.num_docs)\n",
    "        self.assigned_topics = []\n",
    "\n",
    "    def preprocess_tweets(self, tweets):\n",
    "        \n",
    "        #use from other file\n",
    "        return\n",
    "\n",
    "    def init_assignment(self):\n",
    "\n",
    "        for t, tweet in enumerate(self.docs):\n",
    "            words = tweet.split()\n",
    "            curr_doc_topic = []\n",
    "\n",
    "            for w, word in enumerate(words):\n",
    "                try:\n",
    "                    rand_topic = np.random.randint(self.k)\n",
    "                    word_id = self.word2id[word]\n",
    "                    curr_doc_topic.append(rand_topic)\n",
    "                except Exception as e:\n",
    "                    print('Word not in vocab: ', word)\n",
    "                    continue\n",
    "                \n",
    "                self.doc_topic_count[t, rand_topic] += 1\n",
    "                self.topic_word_count[rand_topic, word_id] += 1\n",
    "                self.topic_total_counts[rand_topic] += 1\n",
    "\n",
    "            self.assigned_topics.append(curr_doc_topic)\n",
    "        return \n",
    "\n",
    "    def count_modifier(self, tweet_idx, word_idx, topic_idx, type : str):\n",
    "\n",
    "        var = 0\n",
    "\n",
    "        if type == 'increment':\n",
    "            var = 1\n",
    "        elif type == 'decrement':\n",
    "            var = -1\n",
    "\n",
    "        self.doc_topic_count[tweet_idx, topic_idx] += var\n",
    "        self.topic_word_count[topic_idx, word_idx] += var\n",
    "        self.topic_total_counts[topic_idx] += var\n",
    "    \n",
    "    def gibbs_sampling(self):\n",
    "\n",
    "        # Loop through each word of each document\n",
    "      \n",
    "        # For each word, do the gibbs sampling update equation to get the new probabilities\n",
    "        # then sample based on those proabilities and update the counts\n",
    "        # Update the topic counts\n",
    "        # Repeat for a number of iterations\n",
    "\n",
    "        # first_part = (num words in d that belong to topic t + alpha) / (num words in d + k*alpha)\n",
    "        # second_part = (num of word being assigned to topic k + beta) / (num of words assigned to topic k + V*beta)\n",
    "        # prob(word of doc belongs to topic) = first_part * second_part\n",
    "\n",
    "        for i in tqdm(range(self.iterations)):\n",
    "            \n",
    "            print(f'i : {i}')\n",
    "\n",
    "\n",
    "            for t, tweet in enumerate(self.docs):\n",
    "                words = tweet.split()\n",
    "                for w, word in enumerate(words):\n",
    "                    try:\n",
    "                        old_topic = self.assigned_topics[t][w]\n",
    "                        \n",
    "                        word_id = self.word2id[word]\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        \n",
    "                    #decrement counts to ensure curr word is not considered while choosing new topic\n",
    "                    self.count_modifier(t,word_id,old_topic,'decrement')\n",
    "\n",
    "                    doc_top_frac = self.doc_topic_count[t] + self.alpha\n",
    "                    top_word_frac = self.topic_word_count[:, word_id] + self.beta\n",
    "                    new_prob = doc_top_frac * top_word_frac / (self.topic_total_counts + (self.vocab_size * self.beta))\n",
    "                    new_prob /= np.sum(new_prob)\n",
    "\n",
    "                    new_topic = np.random.choice(np.arange(self.k), p=new_prob)\n",
    "\n",
    "                    # increment counts!!\n",
    "                    self.count_modifier(t,word_id,new_topic,'increment')\n",
    "\n",
    "                    self.assigned_topics[t][w] = new_topic\n",
    "        print('Done.')\n",
    "        return\n",
    "\n",
    "    def pipeline(self):\n",
    "        self.init_assignment()\n",
    "        self.gibbs_sampling()\n",
    "\n",
    "\n",
    "lda_model = LDA(covid_sentences, 7, 12000, 0.1, 0.1)\n",
    "lda_model.pipeline()\n",
    "print(f\"Assigned Topics : {lda_model.assigned_topics}\")\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
